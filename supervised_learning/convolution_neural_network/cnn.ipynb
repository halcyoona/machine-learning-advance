{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.2.0-cp38-cp38-manylinux2010_x86_64.whl (516.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 516.3 MB 1.8 kB/s eta 0:00:01    |██▌                             | 40.1 MB 3.0 MB/s eta 0:02:39     |██▉                             | 45.3 MB 319 kB/s eta 0:24:34     |████▉                           | 78.8 MB 448 kB/s eta 0:16:16     |██████▉                         | 111.0 MB 476 kB/s eta 0:14:12     |███████▉                        | 126.6 MB 718 kB/s eta 0:09:03     |████████▏                       | 131.3 MB 208 kB/s eta 0:30:47     |█████████▉                      | 158.5 MB 723 kB/s eta 0:08:15     |██████████                      | 161.3 MB 818 kB/s eta 0:07:14     |██████████▋                     | 171.3 MB 2.7 MB/s eta 0:02:10     |████████████                    | 192.2 MB 717 kB/s eta 0:07:32     |████████████▎                   | 197.5 MB 668 kB/s eta 0:07:58     |████████████▊                   | 205.0 MB 25 kB/s eta 3:21:57     |█████████████▍                  | 215.5 MB 1.4 MB/s eta 0:03:35     |█████████████▌                  | 218.2 MB 447 kB/s eta 0:11:07     |███████████████▊                | 254.5 MB 670 kB/s eta 0:06:31     |████████████████▍               | 264.5 MB 503 kB/s eta 0:08:20     |████████████████▋               | 267.8 MB 584 kB/s eta 0:07:05     |████████████████▉               | 271.9 MB 485 kB/s eta 0:08:24     |█████████████████               | 273.3 MB 298 kB/s eta 0:13:33     |█████████████████▏              | 276.8 MB 595 kB/s eta 0:06:43     |█████████████████▏              | 277.5 MB 659 kB/s eta 0:06:02     |██████████████████▊             | 301.8 MB 477 kB/s eta 0:07:29     |███████████████████▍            | 313.0 MB 1.3 MB/s eta 0:02:42kB/s eta 2:09:42     |████████████████████▎           | 327.1 MB 824 kB/s eta 0:03:50     |████████████████████▊           | 333.5 MB 33 kB/s eta 1:31:55     |████████████████████▉           | 335.3 MB 33 kB/s eta 1:30:59     |████████████████████▉           | 336.0 MB 33 kB/s eta 1:30:40     |█████████████████████▊          | 350.9 MB 374 kB/s eta 0:07:22     |███████████████████████         | 372.6 MB 224 kB/s eta 0:10:42     |██████████████████████████▏     | 422.1 MB 707 kB/s eta 0:02:14     |██████████████████████████▋     | 429.0 MB 278 kB/s eta 0:05:13     |██████████████████████████▉     | 432.4 MB 1.1 MB/s eta 0:01:19     |██████████████████████████▉     | 433.6 MB 616 kB/s eta 0:02:15     |███████████████████████████     | 436.8 MB 981 kB/s eta 0:01:21��█████████    | 453.2 MB 552 kB/s eta 0:01:55     |█████████████████████████████▍  | 473.9 MB 357 kB/s eta 0:01:59��█████████████████████████▋  | 478.6 MB 33 kB/s eta 0:18:32 31 kB/s eta 0:12:205 MB 1.2 MB/s eta 0:00:16 915 kB/s eta 0:00:19 312 kB/s eta 0:00:498 MB 312 kB/s eta 0:00:47��█████▏| 503.1 MB 454 kB/s eta 0:00:30     |███████████████████████████████▏| 503.2 MB 454 kB/s eta 0:00:29��███████████████▎| 504.0 MB 46 kB/s eta 0:04:26████▍| 507.2 MB 628 kB/s eta 0:00:15███████▊| 511.3 MB 682 kB/s eta 0:00:08     |███████████████████████████████▉| 513.4 MB 463 kB/s eta 0:00:07\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/google-pasta/\u001b[0m\n",
      "\u001b[?25hCollecting google-pasta>=0.1.8\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 1.2 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /home/halcyoona/.local/lib/python3.8/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.14.0)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.30.0-cp38-cp38-manylinux2010_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.1.0\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Downloading absl-py-0.9.0.tar.gz (104 kB)\n",
      "\u001b[K     |████████████████████████████████| 104 kB 9.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /home/halcyoona/.local/lib/python3.8/site-packages (from tensorflow) (1.18.4)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /home/halcyoona/.local/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Collecting protobuf>=3.8.0\n",
      "  Downloading protobuf-3.12.2-cp38-cp38-manylinux1_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 387 kB/s eta 0:00:01     |██████████████▊                 | 583 kB 6.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/lib/python3/dist-packages (from tensorflow) (0.34.2)\n",
      "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
      "\u001b[K     |████████████████████████████████| 454 kB 1.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorboard<2.3.0,>=2.2.0\n",
      "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 444 kB/s eta 0:00:01��█████████████████▏ | 2.8 MB 444 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.2.1-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /home/halcyoona/.local/lib/python3.8/site-packages (from tensorflow) (1.4.1)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from protobuf>=3.8.0->tensorflow) (45.2.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/lib/python3/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (0.16.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.22.0)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "\u001b[K     |████████████████████████████████| 779 kB 6.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.2.2-py3-none-any.whl (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 1.5 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.18.0-py2.py3-none-any.whl (90 kB)\n",
      "\u001b[K     |████████████████████████████████| 90 kB 4.1 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 7.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/lib/python3/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/halcyoona/.local/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.8)\n",
      "Building wheels for collected packages: termcolor, absl-py\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=70b42a5781a9ae0658be34f688982ef36106433b2150615897db3367f4466e8a\n",
      "  Stored in directory: /home/halcyoona/.cache/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121931 sha256=6aa6f8383a07c78dc706f2dcb616f85f7a1d38de771e198f80f350641797b6a2\n",
      "  Stored in directory: /home/halcyoona/.cache/pip/wheels/1d/10/8e/2f79b924179ff1e6510933d63eb851bea01054fff262343b7a\n",
      "Successfully built termcolor absl-py\n",
      "Installing collected packages: google-pasta, termcolor, grpcio, keras-preprocessing, gast, absl-py, protobuf, tensorflow-estimator, astunparse, requests-oauthlib, pyasn1-modules, cachetools, google-auth, google-auth-oauthlib, tensorboard-plugin-wit, markdown, tensorboard, opt-einsum, tensorflow\n",
      "Successfully installed absl-py-0.9.0 astunparse-1.6.3 cachetools-4.1.1 gast-0.3.3 google-auth-1.18.0 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.30.0 keras-preprocessing-1.1.2 markdown-3.2.2 opt-einsum-3.2.1 protobuf-3.12.2 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 tensorboard-2.2.2 tensorboard-plugin-wit-1.7.0 tensorflow-2.2.0 tensorflow-estimator-2.2.0 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras.dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d74421eda2c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1337\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#for reproducibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.dataset'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  #for reproducibility\n",
    "\n",
    "from keras.dataset import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from Keras.utils import np_utils\n",
    "\n",
    "batch_size = 128\n",
    "np_classes = 10 \n",
    "np_epoch = 20\n",
    "\n",
    "#data shuffled and split between train and test sets\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "#reshaping just for the model to run\n",
    "#28x28 = 784\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# normalizing from 0 to 1\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], \"Train Samples\")\n",
    "print(X_test.shape[0], \"Test Samples\")\n",
    "\n",
    "\n",
    "#convert class vectors into binary class matrices\n",
    "Y_train = np_utils.to_categorical(Y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(Y_test, nb_classes)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(512, input_shape=(784, )))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(10))\n",
    "\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "rms = RMSprop()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=rms)\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=2, validation_data=(X_test, Y_test))\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, show_accuracy, verbose=0)\n",
    "print(\"Test score: \", score[0])\n",
    "print(\"Test accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  #for reproducibility\n",
    "\n",
    "from keras.dataset import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolutional2D, MaxPooling\n",
    "from keras.optimizer=rms \n",
    "\n",
    "model.fit(X_train, Y_train)import SGD, Adam, RMSprop\n",
    "from Keras.utils import np_utils\n",
    "\n",
    "batch_size = 128\n",
    "nb_classes = 10 \n",
    "nb_epoch = 20\n",
    "\n",
    "#image dimension\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "#number of convolutional filters to use\n",
    "nb_filters = 32\n",
    "\n",
    "#size of pooling area for max pooling\n",
    "nb_pool\n",
    "\n",
    "#convolutional kernel size \n",
    "nb_conv = 3\n",
    "\n",
    "\n",
    "\n",
    "#data shuffled and split between train and test sets\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "X_train = X_train.reshape(X_train,shape[0], 1, img_rows, img_cols)\n",
    "X_test = X_test.reshape(X_train,shape[0], 1, img_rows, img_cols)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# normalizing from 0 to 1\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], \"Train Samples\")\n",
    "print(X_test.shape[0], \"Test Samples\")\n",
    "\n",
    "\n",
    "#convert class vectors into binary class matrices\n",
    "Y_train = np_utils.to_categorical(Y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(Y_test, nb_classes)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolutional2D(nb_filters, nb_conv, nb_conv, border_mode='valid', input_shape=(1,img_rows,img_cols)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Convolutional2D(nb_filters, nb_conv, nb_conv))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling(pool_size=(nb_pool, nb_pool)))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Drop(0.5))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam)\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=2, validation_data=(X_test, Y_test))\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, show_accuracy, verbose=0)\n",
    "print(\"Test score: \", score[0])\n",
    "print(\"Test accuracy: \", score[1])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
